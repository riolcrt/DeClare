{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data.datasets import DeClareDataset\n",
    "from model.deClare import DeClareModel\n",
    "from metrics.evaluation import DeClareEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOPES_LOC = \"./Datasets/Snopes/snopes.tsv\"\n",
    "#consists of rumors analyzed on the Snopes website along with their credibility labels (true or false), \n",
    "#sets of reporting articles, and their respective web sources\n",
    "\n",
    "POLITIFACT_LOC = \"./Datasets/PolitiFact/politifact.tsv\"\n",
    "\n",
    "glove_data_file = \"./Glove/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snopes = DeClareDataset(SNOPES_LOC, glove_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snopes.news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_lstm_units = 64\n",
    "random_seed = 42\n",
    "\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "shuffle_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(snopes)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "val_split = int(np.floor(val_split * dataset_size))\n",
    "test_split = val_split + int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[test_split:], indices[:val_split], indices[val_split:test_split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(snopes, batch_size, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(snopes, len(val_indices), sampler=val_sampler)\n",
    "test_dataloader = DataLoader(snopes, len(test_indices), sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "declare = DeClareModel(snopes.initial_embeddings, snopes.claim_source_vocab_size, \n",
    "                       snopes.article_source_vocab_size, nb_lstm_units, device)\n",
    "\n",
    "declare.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(declare.parameters(), lr=0.005)\n",
    "BCEloss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "num_epochs = 20\n",
    "reg_lambda = 0.0002\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses[epoch] = []\n",
    "    for data_sample in tqdm(train_dataloader):\n",
    "        \n",
    "        declare.zero_grad()\n",
    "        idx = np.argsort(-data_sample[3])\n",
    "\n",
    "        for i in range(len(data_sample)):\n",
    "            data_sample[i] = data_sample[i][idx].to(device)\n",
    "\n",
    "        out = declare(data_sample[0], data_sample[1], data_sample[2], data_sample[3], data_sample[4], data_sample[5])\n",
    "        unsqueezed_data = data_sample[6].float().unsqueeze(-1)\n",
    "        loss = BCEloss(out, unsqueezed_data)\n",
    "\n",
    "        l2_reg = None\n",
    "        for param in declare.named_parameters():\n",
    "            if 'dense' in param[0] and 'weight' in param[0]:\n",
    "                if l2_reg is None:\n",
    "                    l2_reg = param[1].norm(2)\n",
    "                else:\n",
    "                    l2_reg = l2_reg + param[1].norm(2)\n",
    "\n",
    "        total_loss = loss + reg_lambda*l2_reg\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('total loss', total_loss, epoch)\n",
    "        writer.add_scalar('regularization loss', l2_reg, epoch)\n",
    "        writer.add_scalar('loss', loss, epoch)\n",
    "        writer.add_scalar('pad embedding', declare.word_embeddings(torch.tensor([0]).to(device)).data.mean())\n",
    "        \n",
    "        losses[epoch].append(total_loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(declare.state_dict(), 'demo_model')\n",
    "# declare.load_state_dict(torch.load('demo_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snopes_eval = DeClareEvaluation(declare, test_dataloader, device)\n",
    "labels, preds = snopes_eval.claim_wise_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_claim_indices = np.where(labels==1)\n",
    "false_claim_indices = np.where(labels==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels[true_claim_indices], preds[true_claim_indices]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels[false_claim_indices], preds[false_claim_indices]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potifact_df = pd.read_csv(POLITIFACT_LOC, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potifact_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snopes_df = pd.read_csv(SNOPES_LOC, sep='\\t', header=None)\n",
    "snopes_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0ef8e78c632eda8ebbbac8f404ba1444849ed1d58c6886e8f1a442f11a282a1ea",
   "display_name": "Python 3.7.10 64-bit ('declare': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}