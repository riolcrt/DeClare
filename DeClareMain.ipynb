{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor, optim, cuda\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from data.datasets import DeClareDataset\n",
    "from model.deClare import DeClareModel\n",
    "from metrics.evaluation import DeClareEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOPES_LOC = \"./Datasets/Snopes/snopes.tsv\"\n",
    "#consists of rumors analyzed on the Snopes website along with their credibility labels (true or false), \n",
    "#sets of reporting articles, and their respective web sources\n",
    "\n",
    "POLITIFACT_LOC = \"./Datasets/PolitiFact/politifact.tsv\"\n",
    "\n",
    "glove_data_file = \"./Glove/glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully read news data from ./Datasets/Snopes/snopes.tsv\n",
      "Number of articles = 29242\n",
      "Number of claims = 4341\n",
      "Using pre-built vocabulary\n"
     ]
    }
   ],
   "source": [
    "snopes = DeClareDataset(SNOPES_LOC, glove_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Credibility                Claim_Source  \\\n",
       "0        true  politics_christmas_bestbuy   \n",
       "1        true  politics_christmas_bestbuy   \n",
       "2        true  politics_christmas_bestbuy   \n",
       "3        true  politics_christmas_bestbuy   \n",
       "4        true  politics_christmas_bestbuy   \n",
       "\n",
       "                                               Claim  \\\n",
       "0  best buy chain eschewing use word christmas 20...   \n",
       "1  best buy chain eschewing use word christmas 20...   \n",
       "2  best buy chain eschewing use word christmas 20...   \n",
       "3  best buy chain eschewing use word christmas 20...   \n",
       "4  best buy chain eschewing use word christmas 20...   \n",
       "\n",
       "                                             Article  \\\n",
       "0  there are several holidays throughout that tim...   \n",
       "1  defenses against same photographs show images ...   \n",
       "2  the best that life could think out we extended...   \n",
       "3  this entry november 19 2006 published 9 years ...   \n",
       "4  place he will not do either said snape the ord...   \n",
       "\n",
       "               Article_Source  \n",
       "0  www.godlikeproductions.com  \n",
       "1               www.sjpba.net  \n",
       "2           www.englisher.net  \n",
       "3                    rss2.com  \n",
       "4           www.englisher.net  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Credibility</th>\n      <th>Claim_Source</th>\n      <th>Claim</th>\n      <th>Article</th>\n      <th>Article_Source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>true</td>\n      <td>politics_christmas_bestbuy</td>\n      <td>best buy chain eschewing use word christmas 20...</td>\n      <td>there are several holidays throughout that tim...</td>\n      <td>www.godlikeproductions.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>true</td>\n      <td>politics_christmas_bestbuy</td>\n      <td>best buy chain eschewing use word christmas 20...</td>\n      <td>defenses against same photographs show images ...</td>\n      <td>www.sjpba.net</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>true</td>\n      <td>politics_christmas_bestbuy</td>\n      <td>best buy chain eschewing use word christmas 20...</td>\n      <td>the best that life could think out we extended...</td>\n      <td>www.englisher.net</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>true</td>\n      <td>politics_christmas_bestbuy</td>\n      <td>best buy chain eschewing use word christmas 20...</td>\n      <td>this entry november 19 2006 published 9 years ...</td>\n      <td>rss2.com</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>true</td>\n      <td>politics_christmas_bestbuy</td>\n      <td>best buy chain eschewing use word christmas 20...</td>\n      <td>place he will not do either said snape the ord...</td>\n      <td>www.englisher.net</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "snopes.news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_lstm_units = 64\n",
    "random_seed = 42\n",
    "\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "shuffle_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(snopes)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "val_split = int(np.floor(val_split * dataset_size))\n",
    "test_split = val_split + int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices, test_indices = indices[test_split:], indices[:val_split], indices[val_split:test_split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(snopes, batch_size, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(snopes, len(val_indices), sampler=val_sampler)\n",
    "test_dataloader = DataLoader(snopes, len(test_indices), sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeClareModel(\n",
       "  (word_embeddings): Embedding(56755, 100)\n",
       "  (claim_source_embeddings): Embedding(4342, 4)\n",
       "  (article_source_embeddings): Embedding(12237, 8)\n",
       "  (attention_dense): Linear(in_features=200, out_features=1, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (dense_1): Linear(in_features=140, out_features=64, bias=True)\n",
       "  (dense_1_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (dense_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (dense_2_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (biLSTM): LSTM(100, 64, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "declare = DeClareModel(snopes.initial_embeddings, snopes.claim_source_vocab_size, \n",
    "                       snopes.article_source_vocab_size, nb_lstm_units, device)\n",
    "\n",
    "declare.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(declare.parameters(), lr=0.005)\n",
    "BCEloss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully read news data from ./Datasets/Snopes/snopes.tsv\n",
      "Number of articles = 29242\n",
      "Number of claims = 4341\n",
      "Using pre-built vocabulary\n",
      "\n",
      "  0%|          | 0/366 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/366 [00:01<09:02,  1.48s/it]\u001b[A\n",
      "  1%|          | 2/366 [00:04<13:56,  2.30s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-6b3cdf890916>\", line 9, in <module>\n",
      "    for data_sample in tqdm(train_dataloader):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\tqdm\\notebook.py\", line 239, in __init__\n",
      "    self.container = self.status_printer(self.fp, total, self.desc, self.ncols)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\tqdm\\notebook.py\", line 112, in status_printer\n",
      "    raise ImportError(\n",
      "ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Python39\\lib\\inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Python39\\lib\\inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Python39\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Python39\\lib\\inspect.py\", line 746, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\utils\\shimmodule.py\", line 92, in __getattr__\n",
      "    return import_item(name)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\IPython\\utils\\importstring.py\", line 31, in import_item\n",
      "    module = __import__(package, fromlist=[obj])\n",
      "  File \"c:\\Users\\riolc\\source\\repos\\DeClare\\notebook.py\", line 145, in <module>\n",
      "    total_loss.backward()\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"C:\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 145, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "Successfully read news data from ./Datasets/Snopes/snopes.tsv\n",
      "Number of articles = 29242\n",
      "Number of claims = 4341\n",
      "Using pre-built vocabulary\n",
      "\n",
      "  0%|          | 0/366 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/366 [00:01<08:39,  1.42s/it]\u001b[A\n",
      "  1%|          | 2/366 [00:02<09:00,  1.49s/it]\u001b[A\n",
      "  1%|          | 3/366 [00:04<08:58,  1.48s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "losses = {}\n",
    "num_epochs = 20\n",
    "reg_lambda = 0.0002\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses[epoch] = []\n",
    "    for data_sample in tqdm(train_dataloader):\n",
    "        \n",
    "        declare.zero_grad()\n",
    "        idx = np.argsort(-data_sample[3])\n",
    "\n",
    "        for i in range(len(data_sample)):\n",
    "            data_sample[i] = data_sample[i][idx].to(device)\n",
    "\n",
    "        out = declare(data_sample[0], data_sample[1], data_sample[2], data_sample[3], data_sample[4], data_sample[5])\n",
    "        loss = BCEloss(out, data_sample[6].float())\n",
    "\n",
    "        l2_reg = None\n",
    "        for param in declare.named_parameters():\n",
    "            if 'dense' in param[0] and 'weight' in param[0]:\n",
    "                if l2_reg is None:\n",
    "                    l2_reg = param[1].norm(2)\n",
    "                else:\n",
    "                    l2_reg = l2_reg + param[1].norm(2)\n",
    "\n",
    "        total_loss = loss + reg_lambda*l2_reg\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('total loss', total_loss, epoch)\n",
    "        writer.add_scalar('regularization loss', l2_reg, epoch)\n",
    "        writer.add_scalar('loss', loss, epoch)\n",
    "        writer.add_scalar('pad embedding', declare.word_embeddings(torch.tensor([0]).to(device)).data.mean())\n",
    "        \n",
    "        losses[epoch].append(total_loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(declare.state_dict(), 'demo_model')\n",
    "# declare.load_state_dict(torch.load('demo_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snopes_eval = DeClareEvaluation(declare, test_dataloader, device)\n",
    "labels, preds = snopes_eval.claim_wise_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_claim_indices = np.where(labels==1)\n",
    "false_claim_indices = np.where(labels==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels[true_claim_indices], preds[true_claim_indices]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels[false_claim_indices], preds[false_claim_indices]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potifact_df = pd.read_csv(POLITIFACT_LOC, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potifact_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snopes_df = pd.read_csv(SNOPES_LOC, sep='\\t', header=None)\n",
    "snopes_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}